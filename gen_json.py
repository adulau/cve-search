#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Search the CVE full text database
#
# Software is free software released under the "Modified BSD license"
#
# Copyright (c) 2012-2014 Alexandre Dulaunoy - a@foo.be
# Copyright (c) 2015      Psychedelys

import os
import sys
import json
from bson import json_util
import argparse
import configparser
runPath = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(runPath, "./lib/"))

from whoosh import index
from whoosh.index import exists_in
from whoosh.fields import *
schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT)

# config
Config = configparser.ConfigParser()
Config.read(os.path.join(runPath, "./configuration.ini"))
# default config
indexpath = os.path.join(runPath, "indexdir")
if ('IndexPath' in Config['FulltextIndexer']) and (Config.get('FulltextIndexer', 'IndexPath')!=""):
    indexpath = Config.get('FulltextIndexer', 'IndexPath')
    if not indexpath.startswith('/'):
        indexpath = os.path.join(runPath, indexpath)

weboutput = os.path.join(runPath, "web/json" )

argParser = argparse.ArgumentParser(description='Search for vulnerabilities in the National Vulnerability DB. Data from http://nvd.nist.org.')
argParser.add_argument('-f', type=str, help='F = filter to be used: (all, whitelist, )')
args = argParser.parse_args()

if not os.path.exists(indexpath) or not exists_in(indexpath):
    print("The index does not exist, please launch first the indexer.")
    sys.exit(1)

if not os.path.exists(weboutput):
    try:
        os.mkdir(weboutput)
    except:
        print('not able to create the output directory.')
        sys.exit(1)

param=args.f
if not param:
    param='all'
filename=os.path.join(weboutput,  param + '.json')

ix = index.open_dir(indexpath)

from whoosh.qparser import QueryParser

from whoosh.query import *
import json
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
lmtzr = WordNetLemmatizer()

xr = ix.searcher().reader()
s = {"name": 'cve-search', "children": []}
d = {}
for x in xr.most_frequent_terms("content", 3000):
    query = QueryParser("content", ix.schema).parse(x[1])
    term = lmtzr.lemmatize(x[1].decode('utf-8'), 'v')
    if term in stopwords.words('english'):
        continue
    if term in d:
        d[term]['size'] = d[term]['size']+int(x[0])
    else:
        d[term] = {}
        d[term]['size'] = int(x[0])
for k in sorted(d.keys(), key=lambda y: (d[y]['size']), reverse=True):
    v = {}
    v["name"] = k
    v["size"] = d[k]['size']
    s['children'].append(v)
fjson = open(filename, 'w')
fjson.write(json.dumps(s, indent=4))
fjson.close()
